{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4U7cfbBZa//nnf++sELpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jarekwan/jarwan_projekt/blob/main/PR_KONC_BOOTCAMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBqLoWFVs1Rj"
      },
      "outputs": [],
      "source": [
        "Przygotowanie środowiska i danych\n",
        "Najpierw ustalasz, z jakich źródeł danych korzystasz — z pliku z Kaggle (Supplier Performance Dataset) oraz z danych syntetycznych wygenerowanych samodzielnie w Pythonie.\n",
        "Sprawdzasz, czy dane mają spójne kolumny (np. czas dostawy, niezawodność, koszt, liczba reklamacji).\n",
        "Dane z obu źródeł można analizować osobno albo połączyć w jedną tabelę.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RtUp-R-Ls6iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tkinter import Tk\n",
        "from tkinter.filedialog import askopenfilename\n",
        "\n",
        "# Wczytanie danych z pliku (Supplier Performance Dataset)\n",
        "Tk().withdraw()\n",
        "print(\"Wybierz plik z danymi (Supplier Performance Dataset):\")\n",
        "file_path = askopenfilename()\n",
        "kaggle_data = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Dane z Kaggle wczytane. Wymiary:\", kaggle_data.shape)\n",
        "display(kaggle_data.head())\n",
        "\n",
        "# Wygenerowanie syntetycznych danych w Pythonie\n",
        "synthetic_data = pd.DataFrame({\n",
        "    'supplier_id': [f'S{i}' for i in range(1, 21)],\n",
        "    'delivery_time': np.random.normal(5, 1.5, 20),\n",
        "    'reliability': np.random.uniform(0.7, 1.0, 20),\n",
        "    'cost': np.random.normal(100, 15, 20),\n",
        "    'complaints': np.random.randint(0, 5, 20)\n",
        "})\n",
        "\n",
        "print(\"\\nDane syntetyczne wygenerowane. Wymiary:\", synthetic_data.shape)\n",
        "display(synthetic_data.head())\n",
        "\n",
        "# Sprawdzenie, czy dane mają spójne kolumny\n",
        "print(\"\\nKolumny w danych z Kaggle:\")\n",
        "print(list(kaggle_data.columns))\n",
        "print(\"\\nKolumny w danych syntetycznych:\")\n",
        "print(list(synthetic_data.columns))\n",
        "\n",
        "# Dane z obu źródeł można analizować osobno albo połączyć w jedną tabelę\n",
        "\n"
      ],
      "metadata": {
        "id": "Z2eKr70ws66n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Wstępna analiza danych (EDA)\n",
        "\n",
        "Na tym etapie poznajesz dane:\n",
        "• sprawdzasz, jakie są typy kolumn i czy są braki,\n",
        "• liczysz podstawowe statystyki (średnie, odchylenia),\n",
        "• szukasz wartości odstających,\n",
        "• oglądasz rozkłady i zależności między zmiennymi,\n",
        "• analizujesz korelacje, żeby zobaczyć, które cechy są do siebie podobne lub powielają tę samą informację.\n",
        "Celem EDA jest zrozumienie struktury danych i pierwsze przeczucie, jakie grupy mogą się w nich kryć."
      ],
      "metadata": {
        "id": "mtcWzGT2u7NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Wstępna analiza danych (EDA)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Wybór, które dane analizujemy (można zmienić na synthetic_data)\n",
        "df = kaggle_data.copy()\n",
        "\n",
        "# Podgląd danych\n",
        "print(\"Podgląd danych:\")\n",
        "display(df.head())\n",
        "\n",
        "# Sprawdzenie typów danych i braków\n",
        "print(\"\\nTypy danych:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nLiczba brakujących wartości w kolumnach:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Statystyki opisowe\n",
        "print(\"\\nPodstawowe statystyki:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Wykresy rozkładów dla zmiennych numerycznych\n",
        "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df[num_cols].hist(figsize=(12, 8), bins=20)\n",
        "plt.suptitle(\"Rozkłady zmiennych numerycznych\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Boxploty dla wykrycia wartości odstających\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df[num_cols])\n",
        "plt.title(\"Boxplot – wykrywanie wartości odstających\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Heatmapa korelacji\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df[num_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Macierz korelacji cech\")\n",
        "plt.show()\n",
        "\n",
        "# Analiza – identyfikacja najbardziej skorelowanych zmiennych\n",
        "corr_matrix = df[num_cols].corr().abs()\n",
        "high_corr = corr_matrix.unstack().sort_values(ascending=False)\n",
        "high_corr = high_corr[high_corr < 1].head(5)\n",
        "print(\"\\nNajsilniejsze korelacje między zmiennymi:\")\n",
        "print(high_corr)\n"
      ],
      "metadata": {
        "id": "tUeadRyFu7tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3️⃣ Przygotowanie danych do analizy\n",
        "Na tym etapie porządkujesz dane tak, żeby nadawały się do modelowania:\n",
        "•\tuzupełniasz lub usuwasz brakujące wartości,\n",
        "•\tstandaryzujesz wszystkie cechy (np. żeby koszt i niezawodność były w tej samej skali),\n",
        "•\topcjonalnie możesz usunąć kolumny, które są bardzo silnie skorelowane (bo nie wnoszą nowej informacji).\n",
        "To etap, który sprawia, że PCA i klasteryzacja będą działały prawidłowo.\n"
      ],
      "metadata": {
        "id": "yguVPQqPvtbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Przygotowanie danych do analizy\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Kopia danych\n",
        "data_model = df.copy()\n",
        "\n",
        "# Usuwanie kolumn nienumerycznych (jeśli występują)\n",
        "num_cols = data_model.select_dtypes(include=['float64', 'int64']).columns\n",
        "data_model = data_model[num_cols]\n",
        "\n",
        "# Uzupełnianie braków medianą\n",
        "data_model = data_model.fillna(data_model.median())\n",
        "\n",
        "# Standaryzacja cech (ważne dla PCA i K-Means)\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_model)\n",
        "data_scaled = pd.DataFrame(data_scaled, columns=data_model.columns)\n",
        "\n",
        "print(\"Dane po standaryzacji – pierwsze 5 wierszy:\")\n",
        "display(data_scaled.head())\n",
        "\n",
        "# Opcjonalnie: usunięcie silnie skorelowanych kolumn (>0.9)\n",
        "corr_matrix = data_scaled.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "\n",
        "if len(to_drop) > 0:\n",
        "    print(\"\\nUsuwam silnie skorelowane kolumny:\", to_drop)\n",
        "    data_scaled = data_scaled.drop(columns=to_drop)\n",
        "else:\n",
        "    print(\"\\nBrak bardzo silnie skorelowanych kolumn — nic nie usuwam.\")\n",
        "\n",
        "print(\"\\nOstateczny kształt danych po przygotowaniu:\", data_scaled.shape)\n"
      ],
      "metadata": {
        "id": "XcnF0Q3YvvOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4️⃣ Redukcja wymiarowości (PCA)\n",
        "PCA (Principal Component Analysis) pomaga uprościć dane — czyli zmniejszyć liczbę kolumn, zachowując większość informacji.\n",
        "Dzięki temu:\n",
        "•\tmożesz łatwiej narysować dane na wykresie (np. w 2D),\n",
        "•\tklasteryzacja staje się prostsza i mniej podatna na szum,\n",
        "•\twidać, które kombinacje cech najlepiej różnicują dostawców.\n",
        "Po PCA uzyskasz kilka nowych „składowych”, które zastępują oryginalne kolumny, ale wciąż dobrze opisują zróżnicowanie dostawców.\n"
      ],
      "metadata": {
        "id": "GnuzI1TevyuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4️⃣ Redukcja wymiarowości (PCA)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Uruchomienie PCA – najpierw bez ograniczenia liczby komponentów\n",
        "pca_full = PCA()\n",
        "pca_full.fit(data_scaled)\n",
        "\n",
        "# Analiza wariancji wyjaśnianej\n",
        "explained_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_var) + 1), explained_var, marker='o')\n",
        "plt.title('Wykres skumulowanej wariancji wyjaśnianej przez PCA')\n",
        "plt.xlabel('Liczba komponentów')\n",
        "plt.ylabel('Skumulowana wariancja')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Wybór liczby komponentów (np. tłumaczących 90% wariancji)\n",
        "n_components = np.argmax(explained_var >= 0.9) + 1\n",
        "print(f\"Liczba komponentów PCA tłumaczących ≥90% wariancji: {n_components}\")\n",
        "\n",
        "# Redukcja wymiarów\n",
        "pca = PCA(n_components=n_components)\n",
        "data_pca = pca.fit_transform(data_scaled)\n",
        "\n",
        "# Konwersja do DataFrame\n",
        "data_pca = pd.DataFrame(data_pca, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
        "print(\"\\nDane po redukcji PCA – pierwsze 5 wierszy:\")\n",
        "display(data_pca.head())\n",
        "\n",
        "# Wizualizacja w 2D (pierwsze 2 komponenty)\n",
        "if n_components >= 2:\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    plt.scatter(data_pca['PC1'], data_pca['PC2'], s=30, alpha=0.7)\n",
        "    plt.title('Wizualizacja danych po PCA (2 pierwsze komponenty)')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RlXMX9Q0v6I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5️⃣ Klasteryzacja — K-Means\n",
        "K-Means to metoda, która dzieli dane na określoną liczbę grup.\n",
        "Najpierw sprawdzasz, ile klastrów ma sens — testujesz różne liczby i obserwujesz wykres metody Elbow (gdzie krzywa przestaje gwałtownie spadać).\n",
        "Następnie oceniasz jakość grup za pomocą wskaźnika Silhouette Score, który mówi, jak dobrze punkty pasują do swojej grupy.\n",
        "Po wybraniu najlepszego modelu opisujesz charakterystykę każdej grupy (np. tanio i szybko, drogo i solidnie, tanio ale niestabilnie itd.).\n"
      ],
      "metadata": {
        "id": "sDVgMCWnv7SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Klasteryzacja — K-Means\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Sprawdzenie optymalnej liczby klastrów metodą Elbow\n",
        "wcss = []  # Within Cluster Sum of Squares\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(data_pca)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, wcss, marker='o')\n",
        "plt.title('Metoda Elbow – wybór liczby klastrów K')\n",
        "plt.xlabel('Liczba klastrów (k)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Ocena jakości grup metodą Silhouette\n",
        "silhouette_scores = []\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(data_pca)\n",
        "    score = silhouette_score(data_pca, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, silhouette_scores, marker='o', color='orange')\n",
        "plt.title('Silhouette Score dla różnych k')\n",
        "plt.xlabel('Liczba klastrów (k)')\n",
        "plt.ylabel('Średni Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Wybór najlepszego K (maksymalny Silhouette)\n",
        "best_k = k_range[np.argmax(silhouette_scores)]\n",
        "print(f\"Najlepsza liczba klastrów wg Silhouette Score: {best_k}\")\n",
        "\n",
        "# Finalny model K-Means\n",
        "kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "data_pca['Cluster_KMeans'] = kmeans_final.fit_predict(data_pca)\n",
        "\n",
        "# Podgląd rozkładu klastrów\n",
        "print(\"\\nLiczba obserwacji w każdym klastrze:\")\n",
        "print(data_pca['Cluster_KMeans'].value_counts())\n",
        "\n",
        "# Wizualizacja klastrów w 2D (po PCA)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(\n",
        "    x='PC1', y='PC2',\n",
        "    hue='Cluster_KMeans',\n",
        "    palette='tab10',\n",
        "    data=data_pca,\n",
        "    s=50, alpha=0.8\n",
        ")\n",
        "plt.title('Wizualizacja klastrów K-Means (po PCA)')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HR34xjXLv9m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6️⃣ Klasteryzacja — DBSCAN\n",
        "DBSCAN działa inaczej — nie wymaga podania liczby grup.\n",
        "Sam wykrywa obszary, gdzie punkty są gęsto skupione, i traktuje je jako klastry, a punkty odległe uznaje za „odstające”.\n",
        "Ta metoda jest dobra, jeśli dane są nieregularne lub zawierają pojedynczych, nietypowych dostawców.\n",
        "Porównujesz wyniki DBSCAN z K-Means, by zobaczyć, czy ta metoda lepiej wychwytuje wyjątki lub bardziej naturalne grupy.\n"
      ],
      "metadata": {
        "id": "ORsISXPwwHyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6️⃣ Klasteryzacja — DBSCAN\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Test kilku wartości parametrów eps i min_samples\n",
        "eps_values = [0.3, 0.5, 0.7, 1.0]\n",
        "min_samples_values = [3, 5, 10]\n",
        "\n",
        "best_dbscan = None\n",
        "best_silhouette = -1\n",
        "best_params = None\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_s in min_samples_values:\n",
        "        db = DBSCAN(eps=eps, min_samples=min_s)\n",
        "        labels = db.fit_predict(data_pca.iloc[:, :-1])  # pomijamy kolumnę z klastrem K-Means\n",
        "\n",
        "        # Pomijamy przypadki, gdzie wszystkie punkty są uznane za -1 (szum)\n",
        "        if len(set(labels)) > 1 and len(set(labels)) < len(data_pca):\n",
        "            score = silhouette_score(data_pca.iloc[:, :-1], labels)\n",
        "            if score > best_silhouette:\n",
        "                best_silhouette = score\n",
        "                best_dbscan = db\n",
        "                best_params = (eps, min_s)\n",
        "\n",
        "if best_dbscan:\n",
        "    print(f\"Najlepsze parametry DBSCAN: eps={best_params[0]}, min_samples={best_params[1]}\")\n",
        "    print(f\"Silhouette Score: {best_silhouette:.3f}\")\n",
        "\n",
        "    data_pca['Cluster_DBSCAN'] = best_dbscan.fit_predict(data_pca.iloc[:, :-2])\n",
        "else:\n",
        "    print(\"Nie udało się znaleźć sensownych klastrów DBSCAN (zbyt rozproszone dane).\")\n",
        "\n",
        "# Sprawdzenie liczby grup\n",
        "if 'Cluster_DBSCAN' in data_pca.columns:\n",
        "    print(\"\\nLiczba punktów w każdej grupie (DBSCAN):\")\n",
        "    print(data_pca['Cluster_DBSCAN'].value_counts())\n",
        "\n",
        "    # Wizualizacja klastrów DBSCAN\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(\n",
        "        x='PC1', y='PC2',\n",
        "        hue='Cluster_DBSCAN',\n",
        "        palette='tab10',\n",
        "        data=data_pca,\n",
        "        s=50, alpha=0.8\n",
        "    )\n",
        "    plt.title('Wizualizacja klastrów DBSCAN (po PCA)')\n",
        "    plt.legend(title='Cluster')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "xGDNfLYawMJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7️⃣ Ocena i wizualizacja wyników\n",
        "Na tym etapie porównujesz oba podejścia (K-Means i DBSCAN).\n",
        "Tworzysz wizualizacje po PCA, żeby zobaczyć klastry w układzie 2D lub 3D.\n",
        "Analizujesz, jak średnie wartości cech różnią się między grupami — które grupy mają najniższy koszt, które najwięcej reklamacji itp.\n",
        "Interpretujesz wyniki w kontekście biznesowym: co oznaczają poszczególne grupy i jak mogą pomóc w zarządzaniu dostawcami.\n"
      ],
      "metadata": {
        "id": "1Zr3roGQxc1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7️⃣ Ocena i wizualizacja wyników\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# --- 7.0 Zbiór pomocniczy: standaryzowane cechy jako DataFrame (jeśli jeszcze go nie masz)\n",
        "# Załóżmy, że masz: X_scaled (np.array) i num_cols (lista cech numerycznych)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=num_cols, index=df_clean.index)\n",
        "\n",
        "# --- 7.1 Porównanie jakości: Silhouette (K-Means vs DBSCAN) – jeśli masz zapisane wartości, pokażmy je\n",
        "try:\n",
        "    print(f\"[INFO] K-Means: k={best_k} | silhouette={best_kmeans_sil:.3f}\")\n",
        "except:\n",
        "    print(\"[INFO] Brak zapisanych metryk dla K-Means (best_k / best_kmeans_sil).\")\n",
        "\n",
        "if 'Cluster_DBSCAN' in data_pca.columns:\n",
        "    labels_db = data_pca['Cluster_DBSCAN'].values\n",
        "    # Silhouette liczymy tylko gdy jest co najmniej 2 klastry i nie wszystkie to szum (-1)\n",
        "    if len(set(labels_db)) > 1 and not (set(labels_db) == {-1}):\n",
        "        sil_db = silhouette_score(data_pca[['PC1', 'PC2']], labels_db)\n",
        "        print(f\"[INFO] DBSCAN: clusters={len(set(labels_db))} | silhouette={sil_db:.3f}\")\n",
        "    else:\n",
        "        print(\"[INFO] DBSCAN: brak sensownych klastrów do obliczenia silhouette.\")\n",
        "else:\n",
        "    print(\"[INFO] Brak kolumny 'Cluster_DBSCAN' w data_pca.\")\n",
        "\n",
        "# --- 7.2 Wizualizacje 2D po PCA dla obu metod\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "sns.scatterplot(\n",
        "    data=data_pca, x='PC1', y='PC2', hue='Cluster_KMeans',\n",
        "    palette='tab10', s=50, ax=axes[0], alpha=0.85\n",
        ")\n",
        "axes[0].set_title('K-Means clusters (PCA 2D)')\n",
        "axes[0].legend(title='Cluster', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "\n",
        "if 'Cluster_DBSCAN' in data_pca.columns:\n",
        "    sns.scatterplot(\n",
        "        data=data_pca, x='PC1', y='PC2', hue='Cluster_DBSCAN',\n",
        "        palette='tab10', s=50, ax=axes[1], alpha=0.85\n",
        "    )\n",
        "    axes[1].set_title('DBSCAN clusters (PCA 2D)')\n",
        "    axes[1].legend(title='Cluster', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "else:\n",
        "    axes[1].axis('off')\n",
        "    axes[1].set_title('DBSCAN: no labels')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 7.3 Charakterystyka klastrów: średnie cech (w skali standaryzowanej)\n",
        "def cluster_profile_table(labels_col_name):\n",
        "    \"\"\"\n",
        "    Buduje tabelę średnich cech (z-score) per klaster oraz liczności.\n",
        "    labels_col_name: 'Cluster_KMeans' lub 'Cluster_DBSCAN'\n",
        "    \"\"\"\n",
        "    if labels_col_name not in data_pca.columns:\n",
        "        return None\n",
        "\n",
        "    labels = data_pca[labels_col_name]\n",
        "    tmp = df_scaled.copy()\n",
        "    tmp[labels_col_name] = labels.values  # dopasowanie indeksu\n",
        "\n",
        "    # Usuwamy szum DBSCAN (-1) z profilu, ale pokazujemy liczność\n",
        "    if labels_col_name == 'Cluster_DBSCAN':\n",
        "        counts = tmp[labels_col_name].value_counts().sort_index()\n",
        "        prof = tmp[tmp[labels_col_name] != -1].groupby(labels_col_name)[num_cols].mean().round(2)\n",
        "    else:\n",
        "        counts = tmp[labels_col_name].value_counts().sort_index()\n",
        "        prof = tmp.groupby(labels_col_name)[num_cols].mean().round(2)\n",
        "\n",
        "    return prof, counts\n",
        "\n",
        "# K-Means profile\n",
        "km_prof, km_counts = cluster_profile_table('Cluster_KMeans')\n",
        "print(\"\\n[INFO] K-Means — liczność klastrów:\")\n",
        "print(km_counts)\n",
        "print(\"\\n[INFO] K-Means — profil (średnie z-score cech):\")\n",
        "display(km_prof)\n",
        "\n",
        "# DBSCAN profile (jeśli są etykiety)\n",
        "if 'Cluster_DBSCAN' in data_pca.columns:\n",
        "    db_prof, db_counts = cluster_profile_table('Cluster_DBSCAN')\n",
        "    print(\"\\n[INFO] DBSCAN — liczność klastrów (uwzględnia szum -1):\")\n",
        "    print(db_counts)\n",
        "    if db_prof is not None and not db_prof.empty:\n",
        "        print(\"\\n[INFO] DBSCAN — profil (średnie z-score cech, bez szumu -1):\")\n",
        "        display(db_prof)\n",
        "\n",
        "# --- 7.4 Heatmapy profili cech per klaster (ułatwia „czytanie” grup)\n",
        "def plot_cluster_heatmap(profile_df, title):\n",
        "    if profile_df is None or profile_df.empty:\n",
        "        print(f\"[WARN] {title}: brak danych do heatmapy.\")\n",
        "        return\n",
        "    plt.figure(figsize=(min(12, 1.2*len(profile_df.columns)), 0.7*len(profile_df)))\n",
        "    sns.heatmap(profile_df, cmap='vlag', center=0, annot=True, fmt='.2f')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Features (z-score)')\n",
        "    plt.ylabel('Cluster')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_cluster_heatmap(km_prof, 'K-Means: cluster profile (z-score means)')\n",
        "if 'Cluster_DBSCAN' in data_pca.columns and db_prof is not None and not db_prof.empty:\n",
        "    plot_cluster_heatmap(db_prof, 'DBSCAN: cluster profile (z-score means, noise removed)')\n",
        "\n",
        "# --- 7.5 Krótka, automatyczna etykietyzacja klastrów (przykład)\n",
        "# Heurystyka: wskaż cechy o najwyższym bezwzględnym z-score w danym klastrze\n",
        "def auto_labels_from_profile(profile_df, top_n=2):\n",
        "    if profile_df is None or profile_df.empty:\n",
        "        return {}\n",
        "    labels = {}\n",
        "    for c in profile_df.index:\n",
        "        top_feats = profile_df.loc[c].abs().sort_values(ascending=False).head(top_n).index.tolist()\n",
        "        sign = ['+' if profile_df.loc[c, f] > 0 else '-' for f in top_feats]\n",
        "        pretty = ', '.join([f\"{f}({s})\" for f, s in zip(top_feats, sign)])\n",
        "        labels[c] = pretty\n",
        "    return labels\n",
        "\n",
        "km_labels = auto_labels_from_profile(km_prof, top_n=2)\n",
        "print(\"\\n[INFO] Propozycje podpisów K-Means (na podstawie z-score):\")\n",
        "for c, lab in km_labels.items():\n",
        "    print(f\"  • Cluster {c}: {lab}\")\n",
        "\n",
        "if 'Cluster_DBSCAN' in data_pca.columns and db_prof is not None and not db_prof.empty:\n",
        "    db_labels = auto_labels_from_profile(db_prof, top_n=2)\n",
        "    print(\"\\n[INFO] Propozycje podpisów DBSCAN (bez -1):\")\n",
        "    for c, lab in db_labels.items():\n",
        "        print(f\"  • Cluster {c}: {lab}\")\n",
        "\n",
        "# --- 7.6 (Opcja) Rzutowanie wyników z powrotem do oryginalnego df, by łatwo filtrować dostawców\n",
        "df_results = df_clean.copy()\n",
        "df_results[['PC1','PC2']] = data_pca[['PC1','PC2']]\n",
        "df_results['Cluster_KMeans'] = data_pca['Cluster_KMeans'].values\n",
        "if 'Cluster_DBSCAN' in data_pca.columns:\n",
        "    df_results['Cluster_DBSCAN'] = data_pca['Cluster_DBSCAN'].values\n",
        "\n",
        "# Przykładowe „fiszki” biznesowe: top 10 dostawców z klastra o najniższym średnim lead time (K-Means)\n",
        "if km_prof is not None:\n",
        "    try:\n",
        "        lead_col_guess = [c for c in num_cols if 'lead' in c.lower() or 'delivery_time' in c.lower() or 'on_time' in c.lower()]\n",
        "        if lead_col_guess:\n",
        "            # zakładamy, że mniejsza wartość = lepiej (krótszy lead time)\n",
        "            best_cluster = km_prof[lead_col_guess].mean(axis=1).sort_values().index[0]\n",
        "            print(f\"\\n[INFO] K-Means: klaster o najlepszym (najniższym) średnim LEAD TIME to: {best_cluster}\")\n",
        "            sample = df_results[df_results['Cluster_KMeans'] == best_cluster].head(10)\n",
        "            display(sample)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Nie udało się wygenerować 'fiszek' biznesowych: {e}\")\n"
      ],
      "metadata": {
        "id": "pGtLIEy7xeD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8️⃣ Wnioski i interpretacja biznesowa\n",
        "Na końcu opisujesz, co udało się odkryć:\n",
        "•\tjakie grupy dostawców istnieją,\n",
        "•\tktóre są strategiczne, a które ryzykowne,\n",
        "•\tjak firma mogłaby wykorzystać te informacje (np. renegocjacje kontraktów, zmiana priorytetów dostaw, lepsze planowanie zapasów).\n",
        "Celem jest pokazanie, że analiza ma realne zastosowanie, a nie tylko techniczny wynik\n"
      ],
      "metadata": {
        "id": "9l-8CA2iyZZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9KRZXodyZ9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}